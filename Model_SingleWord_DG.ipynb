{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_SingleWord_DG.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBTRuZfpApVdH8RVHfPF9U"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Required installations"
      ],
      "metadata": {
        "id": "P447fREv2LA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzJ8USfkua04"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!python -m spacy download en\n",
        "!pip install bert-extractive-summarizer --upgrade --force-reinstall\n",
        "\n",
        "!pip install -U nltk\n",
        "!pip install -U pywsd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.2.3 --upgrade --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7C9f8g_DugKz",
        "outputId": "dbaa76d5-10be-4d82-e2d1-c131d29f8eee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.2.3\n",
            "  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Using cached preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Using cached spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting wasabi<1.1.0,>=0.8.1\n",
            "  Using cached wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Using cached murmurhash-1.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Using cached thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Using cached cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0\n",
            "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Using cached spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Using cached pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Using cached catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting blis<0.8.0,>=0.4.0\n",
            "  Using cached blis-0.7.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Using cached srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Using cached pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "Collecting tqdm<5.0.0,>=4.38.0\n",
            "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Using cached typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting jinja2\n",
            "  Using cached Jinja2-3.1.1-py3-none-any.whl (132 kB)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
            "Collecting numpy>=1.15.0\n",
            "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Using cached pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "Collecting click<9.0.0,>=7.1.1\n",
            "  Downloading click-8.1.2-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: zipp, typing-extensions, importlib-metadata, numpy, murmurhash, cymem, click, catalogue, wasabi, urllib3, typer, srsly, smart-open, setuptools, pyparsing, pydantic, preshed, MarkupSafe, idna, charset-normalizer, certifi, blis, tqdm, thinc, spacy-loggers, spacy-legacy, requests, pathy, packaging, langcodes, jinja2, spacy\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.8.0\n",
            "    Uninstalling zipp-3.8.0:\n",
            "      Successfully uninstalled zipp-3.8.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.6\n",
            "    Uninstalling murmurhash-1.0.6:\n",
            "      Successfully uninstalled murmurhash-1.0.6\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.6\n",
            "    Uninstalling cymem-2.0.6:\n",
            "      Successfully uninstalled cymem-2.0.6\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.0.4\n",
            "    Uninstalling click-8.0.4:\n",
            "      Successfully uninstalled click-8.0.4\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.7\n",
            "    Uninstalling catalogue-2.0.7:\n",
            "      Successfully uninstalled catalogue-2.0.7\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 0.9.1\n",
            "    Uninstalling wasabi-0.9.1:\n",
            "      Successfully uninstalled wasabi-0.9.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.4.1\n",
            "    Uninstalling typer-0.4.1:\n",
            "      Successfully uninstalled typer-0.4.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.3\n",
            "    Uninstalling srsly-2.4.3:\n",
            "      Successfully uninstalled srsly-2.4.3\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 5.2.1\n",
            "    Uninstalling smart-open-5.2.1:\n",
            "      Successfully uninstalled smart-open-5.2.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 62.1.0\n",
            "    Uninstalling setuptools-62.1.0:\n",
            "      Successfully uninstalled setuptools-62.1.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.8\n",
            "    Uninstalling pyparsing-3.0.8:\n",
            "      Successfully uninstalled pyparsing-3.0.8\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.8.2\n",
            "    Uninstalling pydantic-1.8.2:\n",
            "      Successfully uninstalled pydantic-1.8.2\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.6\n",
            "    Uninstalling preshed-3.0.6:\n",
            "      Successfully uninstalled preshed-3.0.6\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.1\n",
            "    Uninstalling MarkupSafe-2.1.1:\n",
            "      Successfully uninstalled MarkupSafe-2.1.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.3\n",
            "    Uninstalling idna-3.3:\n",
            "      Successfully uninstalled idna-3.3\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.7\n",
            "    Uninstalling blis-0.7.7:\n",
            "      Successfully uninstalled blis-0.7.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.15\n",
            "    Uninstalling thinc-8.0.15:\n",
            "      Successfully uninstalled thinc-8.0.15\n",
            "  Attempting uninstall: spacy-loggers\n",
            "    Found existing installation: spacy-loggers 1.0.2\n",
            "    Uninstalling spacy-loggers-1.0.2:\n",
            "      Successfully uninstalled spacy-loggers-1.0.2\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.9\n",
            "    Uninstalling spacy-legacy-3.0.9:\n",
            "      Successfully uninstalled spacy-legacy-3.0.9\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: pathy\n",
            "    Found existing installation: pathy 0.6.1\n",
            "    Uninstalling pathy-0.6.1:\n",
            "      Successfully uninstalled pathy-0.6.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: langcodes\n",
            "    Found existing installation: langcodes 3.3.0\n",
            "    Uninstalling langcodes-3.3.0:\n",
            "      Successfully uninstalled langcodes-3.3.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.1\n",
            "    Uninstalling Jinja2-3.1.1:\n",
            "      Successfully uninstalled Jinja2-3.1.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.2.4\n",
            "    Uninstalling spacy-3.2.4:\n",
            "      Successfully uninstalled spacy-3.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed MarkupSafe-2.1.1 blis-0.7.7 catalogue-2.0.7 certifi-2021.10.8 charset-normalizer-2.0.12 click-8.1.2 cymem-2.0.6 idna-3.3 importlib-metadata-4.11.3 jinja2-3.1.1 langcodes-3.3.0 murmurhash-1.0.6 numpy-1.21.6 packaging-21.3 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 pyparsing-3.0.8 requests-2.27.1 setuptools-62.1.0 smart-open-5.2.1 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 tqdm-4.64.0 typer-0.4.1 typing-extensions-3.10.0.2 urllib3-1.26.9 wasabi-0.9.1 zipp-3.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flashtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gAQZPmkulI4",
        "outputId": "496adf8d-7c62-480a-9106-af436529a72c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flashtext in /usr/local/lib/python3.7/dist-packages (2.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pywsd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu3r9obsumqn",
        "outputId": "bc27e71c-8758-4c0f-f8ab-e4621394182f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pywsd in /usr/local/lib/python3.7/dist-packages (1.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pywsd) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.3.5)\n",
            "Requirement already satisfied: wn in /usr/local/lib/python3.7/dist-packages (from pywsd) (0.0.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (2022.3.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (8.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click->nltk->pywsd) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk->pywsd) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk->pywsd) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U wn==0.0.22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owJ-IxWLupuk",
        "outputId": "41af6708-69b0-4874-ec78-9b1303f0fe03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wn==0.0.22 in /usr/local/lib/python3.7/dist-packages (0.0.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KprjZU2KurTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import libraries"
      ],
      "metadata": {
        "id": "gb5c67Nwv3Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_bGav0XubjM",
        "outputId": "a675b9b0-5682-4d65-aa87-6655bd6db7ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "import itertools\n",
        "import re\n",
        "import pke\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "zL3rqZACud41"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from wn import WordNet\n",
        "from pywsd.similarity import max_similarity\n",
        "from pywsd.lesk import adapted_lesk\n",
        "from pywsd.lesk import simple_lesk\n",
        "from pywsd.lesk import cosine_lesk\n",
        "from nltk.corpus import wordnet as wn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VXb10PluvVS",
        "outputId": "bc1df8b4-b8ad-4bda-d0fb-e0f753257c05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up PyWSD (takes ~10 secs)... took 6.93432879447937 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from flashtext import KeywordProcessor"
      ],
      "metadata": {
        "id": "H5gUXLYIuxG8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import Summarizer"
      ],
      "metadata": {
        "id": "_mDZrLucuyaY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function definitions"
      ],
      "metadata": {
        "id": "XNcPZVzbv9Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nouns_multipartite(text):\n",
        "    out=[]\n",
        "\n",
        "    extractor = pke.unsupervised.MultipartiteRank()\n",
        "    # extractor = pke.unsupervised.TfIdf()\n",
        "    #    not contain punctuation marks or stopwords as candidates.\n",
        "    # pos = {'PROPN'}\n",
        "    pos = {'VERB', 'ADJ', 'NOUN'}\n",
        "    stoplist = list(string.punctuation)\n",
        "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "    stoplist += stopwords.words('english')\n",
        "\n",
        "    extractor.load_document(input=text,language='en',normalization=None, stoplist=stoplist)\n",
        "    extractor.candidate_selection(pos=pos)\n",
        "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
        "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
        "    #    threshold/method parameters.\n",
        "    extractor.candidate_weighting(alpha=1.1,\n",
        "                                  threshold=0.75,\n",
        "                                  method='average')\n",
        "    keyphrases = extractor.get_n_best(n=20)\n",
        "\n",
        "    for key in keyphrases:\n",
        "        out.append(key[0])\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "KiizmeNvu3YD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = [sent_tokenize(text)]\n",
        "    sentences = [y for x in sentences for y in x]\n",
        "    # Remove any short sentences less than 20 letters.\n",
        "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
        "    return sentences\n",
        "\n",
        "def get_sentences_for_keyword(keywords, sentences):\n",
        "    keyword_processor = KeywordProcessor()\n",
        "    keyword_sentences = {}\n",
        "    for word in keywords:\n",
        "        keyword_sentences[word] = []\n",
        "        keyword_processor.add_keyword(word)\n",
        "    for sentence in sentences:\n",
        "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
        "        for key in keywords_found:\n",
        "            keyword_sentences[key].append(sentence)\n",
        "\n",
        "    for key in keyword_sentences.keys():\n",
        "        values = keyword_sentences[key]\n",
        "        values = sorted(values, key=len, reverse=True)\n",
        "        keyword_sentences[key] = values\n",
        "    return keyword_sentences\n"
      ],
      "metadata": {
        "id": "_CBcqvN2u6RL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distractors from Wordnet\n",
        "def get_distractors_wordnet(syn,word):\n",
        "    distractors=[]\n",
        "    word= word.lower()\n",
        "    orig_word = word\n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    hypernym = syn.hypernyms()\n",
        "    if len(hypernym) == 0: \n",
        "        return distractors\n",
        "    for item in hypernym[0].hyponyms():\n",
        "        name = item.lemmas()[0].name()\n",
        "        #print (\"name \",name, \" word\",orig_word)\n",
        "        if name == orig_word:\n",
        "            continue\n",
        "        name = name.replace(\"_\",\" \")\n",
        "        name = \" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in distractors:\n",
        "            distractors.append(name)\n",
        "    return distractors\n",
        "\n",
        "def get_wordsense(sent,word):\n",
        "    word= word.lower()\n",
        "    \n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    \n",
        "    \n",
        "    synsets = wn.synsets(word,'n')\n",
        "    if synsets:\n",
        "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
        "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
        "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
        "        return synsets[lowest_index]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Distractors from http://conceptnet.io/\n",
        "def get_distractors_conceptnet(word):\n",
        "    word = word.lower()\n",
        "    original_word= word\n",
        "    if (len(word.split())>0):\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    distractor_list = [] \n",
        "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
        "    obj = requests.get(url).json()\n",
        "\n",
        "    for edge in obj['edges']:\n",
        "        link = edge['end']['term'] \n",
        "\n",
        "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
        "        obj2 = requests.get(url2).json()\n",
        "        for edge in obj2['edges']:\n",
        "            word2 = edge['start']['label']\n",
        "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
        "                distractor_list.append(word2)\n",
        "                   \n",
        "    return distractor_list"
      ],
      "metadata": {
        "id": "u1aLbWeou83p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actual code"
      ],
      "metadata": {
        "id": "DtuRMtjZwAvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgcb_BTHyle3",
        "outputId": "70ac4c81-78cf-46a9-f0f8-0c512d87ca33"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def func(model, full_text):\n",
        "  \n",
        "  result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
        "\n",
        "  summarized_text = ''.join(result)\n",
        "  # print (summarized_text)\n",
        "\n",
        "  keywords = get_nouns_multipartite(full_text) \n",
        "  # print (keywords)\n",
        "  filtered_keys=[]\n",
        "  for keyword in keywords:\n",
        "      if keyword.lower() in summarized_text.lower():\n",
        "          filtered_keys.append(keyword)\n",
        "          \n",
        "  # print (filtered_keys)\n",
        "\n",
        "  sentences = tokenize_sentences(summarized_text)\n",
        "  keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n",
        "          \n",
        "  keyword_sentence_mapping\n",
        "\n",
        "  key_distractor_list = {}\n",
        "\n",
        "  for keyword in keyword_sentence_mapping:\n",
        "      wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
        "      if wordsense:\n",
        "          distractors = get_distractors_wordnet(wordsense,keyword)\n",
        "          if len(distractors) ==0:\n",
        "              distractors = get_distractors_conceptnet(keyword)\n",
        "          if len(distractors) != 0:\n",
        "              key_distractor_list[keyword] = distractors\n",
        "      else:\n",
        "          \n",
        "          distractors = get_distractors_conceptnet(keyword)\n",
        "          if len(distractors) != 0:\n",
        "              key_distractor_list[keyword] = distractors\n",
        "\n",
        "\n",
        "  index = 1\n",
        "  result = []\n",
        "  for each in key_distractor_list:\n",
        "      sentence = keyword_sentence_mapping[each][0]\n",
        "      pattern = re.compile(each, re.IGNORECASE)\n",
        "      output = pattern.sub( \" _______ \", sentence)\n",
        "      choices = [each.capitalize()] + key_distractor_list[each]\n",
        "      top4choices = choices[:4]\n",
        "\n",
        "      new_entry = dict()\n",
        "      new_entry['question']=output\n",
        "      new_entry['ans']=each\n",
        "      new_entry['distractor']=top4choices\n",
        "      result.append(new_entry)\n",
        "      index = index + 1\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "C6mW29K2xYTN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func(model, \"Mike and Morris lived in the same village. While Morris owned the largest jewelry shop in the village, Mike was a poor farmer. Both had large families with many sons, daughters-in-law and grandchildren.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXakPiAqxmyt",
        "outputId": "57114306-ed48-4d28-d3f7-fa4160cf11fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'ans': 'daughters-in-law',\n",
              "  'distractor': ['Daughters-in-law',\n",
              "   'Brother-in-law',\n",
              "   'Daughter-in-law',\n",
              "   'Father-in-law'],\n",
              "  'question': 'Both had large families with many sons,  _______  and grandchildren.'},\n",
              " {'ans': 'grandchildren',\n",
              "  'distractor': ['Grandchildren', 'Baby', 'Bastard', 'Child'],\n",
              "  'question': 'Both had large families with many sons, daughters-in-law and  _______ .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# f = open(\"egypt.txt\",\"r\")\n",
        "# full_text = f.read()\n",
        "# full_text = \"One of the most famous people born in Warsaw was Maria Skłodowska-Curie, who achieved international recognition for her research on radioactivity and was the first female recipient of the Nobel Prize. Famous musicians include Władysław Szpilman and Frédéric Chopin. Though Chopin was born in the village of Żelazowa Wola, about 60 km (37 mi) from Warsaw, he moved to the city with his family when he was seven months old. Casimir Pulaski, a Polish general and hero of the American Revolutionary War, was born here in 1745.\"\n",
        "full_text = \"In Chapter 3 you learnt that woollen clothes are made from animal fibres.You also know that cotton clothesare made from plant fibres. We wear woollen clothes during winters when it is cold outside. Woollen clothes keep us warm. We prefer to wear light coloured cotton clothes when it is hot. These give us a feeling of coolness. You might have wondered why particular types of clothes are suitable for a particular season. In winter you feel cold inside the house. If you come out in the sun, you feel warm. In summer, you feel hot even inside the house. How do we know whether an object is hot or cold? How do we find out how hot or cold an object is? In this chapter we shall try to seek answers to some of these questions.In our day-to-day life, we come across a number of objects. Some of them are hot and some of them are cold. Tea is hot and ice is cold. List some objects you use commonly in Table 4.1. Mark these objects as hot or cold.We see that some objects are cold while some are hot. You also know that some objects are hotter than others while some are colder than others. How do we decide which object is hotter than the other? We often do it by touching the objects. But is our sense of touch reliable? Let us find out.A reliable measure of the hotness of an object is its temperature. Temperature is measured by a device called thermometer. Have you seen a thermometer? Recall that when you or someone else in your family had fever, the temperature was measured by a thermometer. The thermometer that measures our body temperature is called a clinical thermometer. Hold the thermometer in your hand and examine it carefully. If you do not have a thermometer, requesta friend to share it with you. A clinical thermometer looks like the one shown in Fig. 4.2. A clinical thermometer consists of a long, narrow, uniform glass tube. It has a bulb at one end. This bulb contains mercury. Outside the bulb, a small shining thread of mercury can be seen. If you do not see the mercury thread, rotate the thermometer a bit till you see it. You will also find a scale on the thermometer. The scale we use is the celsius scale, indicated by °C. A clinical thermometer reads temperature from 35°C to 42°C. Let us learn how to read a thermometer. First, note the temperature difference indicated between the two bigger marks. Also note down the number of divisions (shown by smaller marks) between these marks. Suppose the bigger marks read one degree and there are five divisions between them. Then, one small division can read 1/5 = 0.2°C. Wash the thermometer, preferably with an antiseptic solution. Hold it firmly and give it a few jerks. The jerks will bring the level of mercury down. Ensure that it falls below 35°C. Now place the bulb of the thermometer under your tongue. After one minute, take the thermometer out and note the reading. This is your body temperature. The temperature should always be stated with its unit, °C. The normal temperature of human body is 37°C. Note that the temperature is stated with its unit. The temperature of every person may not be 37°C. It could be slightly higher or slightly lower. Actually, what we call normal temperature is the average body temperature of a large number of healthy persons. The clinical thermometer is designed to measure the temperature of human body only. The temperature of human body normally does not go below 35oC or above 42oC. That is the reason that this thermometer has the range 35oC to 42oC. How do we measure the temperature of other objects? For this purpose, there are other thermometers. One such thermometer is known as the laboratory thermometer. The teacher will show you this thermometer. Look at it carefully and note the highest and the lowest temperature it can measure. The range of a laboratory thermometer is generally from –10°C to 110°C (Fig. 4.4). Also, as you did in the case of the clinical thermometer, find out how much a small division on this thermometer reads. You would need this information to read the thermometer correctly. You might have observed that a frying pan becomes hot when kept on a flame. It is because the heat passes from the flame to the utensil. When the pan is removed from the fire, it slowly cools down. Why does it cool down? The heat is transferred from the pan to the surroundings. So you can understand that in both cases, the heat flows from a hotter object to a colder object. In fact, in all cases heat flows from a hotter object to a colder object. The process by which heat is transferred from the hotter end to the colder end of an object is known as conduction. In solids, generally, the heat is transferred by the process of conduction. The materials which allow heat to pass through them easily are conductors of heat. For examples, aluminum, iron and copper. The materials which do not allow heat to pass through them easily are poor conductors of heat such as plastic and wood. Poor conductors are known as insulators. The water and air are poor conductors of heat.\"\n",
        "model = Summarizer()\n",
        "result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
        "\n",
        "summarized_text = ''.join(result)\n",
        "print (summarized_text)"
      ],
      "metadata": {
        "id": "uZkKGzMYwFch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = tokenize_sentences(summarized_text)\n",
        "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n",
        "        \n",
        "keyword_sentence_mapping"
      ],
      "metadata": {
        "id": "vFaffLY2wMUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_distractor_list = {}\n",
        "\n",
        "for keyword in keyword_sentence_mapping:\n",
        "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
        "    if wordsense:\n",
        "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
        "        if len(distractors) ==0:\n",
        "            distractors = get_distractors_conceptnet(keyword)\n",
        "        if len(distractors) != 0:\n",
        "            key_distractor_list[keyword] = distractors\n",
        "    else:\n",
        "        \n",
        "        distractors = get_distractors_conceptnet(keyword)\n",
        "        if len(distractors) != 0:\n",
        "            key_distractor_list[keyword] = distractors"
      ],
      "metadata": {
        "id": "QlFkk7sTwORW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "print (\"#############################################################################\")\n",
        "print (\"NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \")\n",
        "print (\"#############################################################################\\n\\n\")\n",
        "for each in key_distractor_list:\n",
        "    sentence = keyword_sentence_mapping[each][0]\n",
        "    pattern = re.compile(each, re.IGNORECASE)\n",
        "    output = pattern.sub( \" _______ \", sentence)\n",
        "    print (\"%s)\"%(index),output)\n",
        "    choices = [each.capitalize()] + key_distractor_list[each]\n",
        "    top4choices = choices[:4]\n",
        "    random.shuffle(top4choices)\n",
        "    optionchoices = ['a','b','c','d']\n",
        "    for idx,choice in enumerate(top4choices):\n",
        "        print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
        "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
        "    index = index + 1"
      ],
      "metadata": {
        "id": "NhI7jqvcwQUy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}